{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "\n",
    "from nerfstudio.models.nerfacto import NerfactoModelConfig\n",
    "from nerfstudio.data.scene_box import SceneBox\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self.id:  0  -  0  -  35\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_samples, input_size, target_size, id=0):\n",
    "        self.num_samples = num_samples\n",
    "        self.input_size = input_size\n",
    "        self.target_size = target_size\n",
    "        self.id = id\n",
    "        self.count = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(\"Self.id: \", self.id, \" - \", self.count, \" - \", randrange(50))\n",
    "        self.count += 1\n",
    "        # print(\"Get item called\")\n",
    "        time.sleep(2)\n",
    "        input_data = torch.randn(self.input_size)\n",
    "        target_data = torch.randn(self.target_size)\n",
    "        return str(self.id)\n",
    "\n",
    "# Usage example\n",
    "num_samples = 1000\n",
    "input_size = (3, 224, 224)\n",
    "target_size = (10,)\n",
    "\n",
    "random_dataset = RandomDataset(num_samples, input_size, target_size)\n",
    "\n",
    "datasets = [RandomDataset(num_samples, input_size, target_size, id=i) for i in range(10)]\n",
    "\n",
    "print(random_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableDataset(Dataset):\n",
    "    def __init__(self, iter_datasets):\n",
    "        self.iter_datasets = iter_datasets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.iter_datasets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return next(self.iter_datasets[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousDataLoader:\n",
    "    def __init__(self, dataloader):\n",
    "        self.data_loader = dataloader\n",
    "        self.data_iter = iter(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            return next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            self.data_iter = iter(self.data_loader)\n",
    "            return next(self.data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self.id: Self.id: Self.id: Self.id:     023  1  -   -   -   -   2828 2628   -  29  -  -   6\n",
      "9 - \n",
      " \n",
      "30\n",
      "Self.id:  4  -  26  -  7\n",
      "Self.id:  7 Self.id:  -   266Self.id:    -    - 89  26\n",
      " -    - 26  43 - \n",
      " 37\n",
      "Self.id:  5  -  26  -  31\n",
      "Self.id:  9  -  26  -  21\n",
      "['0']\n",
      "Took  2.1125333309173584  seconds\n",
      "['1']\n",
      "Took  0.0003235340118408203  seconds\n",
      "['2']\n",
      "Took  0.0002429485321044922  seconds\n",
      "['3']\n",
      "Took  0.0002887248992919922  seconds\n",
      "['4']\n",
      "Took  0.00046253204345703125  seconds\n",
      "['5']\n",
      "Took  0.0004875659942626953  seconds\n",
      "['6']\n",
      "Took  0.0002741813659667969  seconds\n",
      "['7']\n",
      "Took  0.0004222393035888672  seconds\n",
      "['8']\n",
      "Took  0.00043845176696777344  seconds\n",
      "['9']\n",
      "Took  0.0006718635559082031  seconds\n",
      "['0']\n",
      "Took  0.02902054786682129  seconds\n",
      "['1']\n",
      "Took  0.00017404556274414062  seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m time2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTook \u001b[39m\u001b[39m\"\u001b[39m, time2 \u001b[39m-\u001b[39m time1, \u001b[39m\"\u001b[39m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iter_datasets = [iter(dataset) for dataset in datasets]\n",
    "meta_dataset = IterableDataset(iter_datasets)\n",
    "meta_dataloader = torch.utils.data.DataLoader(meta_dataset, batch_size=1, shuffle=False, num_workers=5, prefetch_factor=2)\n",
    "from itertools import cycle\n",
    "\n",
    "inifinite_meta_dataloader = cycle(meta_dataloader)\n",
    "\n",
    "while True:\n",
    "    time1 = time.time()\n",
    "    print(next(inifinite_meta_dataloader))\n",
    "    time2 = time.time()\n",
    "    print(\"Took \", time2 - time1, \" seconds\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Queue\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import cycle\n",
    "\n",
    "class PrefetchLoader:\n",
    "    def __init__(self, datasets, batch_size, prefetch_batches):\n",
    "        self.datasets = cycle(datasets)\n",
    "        self.batch_size = batch_size\n",
    "        self.prefetch_batches = prefetch_batches\n",
    "        self.executor = ThreadPoolExecutor(max_workers=prefetch_batches)\n",
    "        self.queue = Queue(maxsize=prefetch_batches)\n",
    "        self.queued = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def prefetch(self):\n",
    "        dataset = next(self.datasets)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "        self.queue.put(next(iter(loader)))\n",
    "        self.queued -= 1\n",
    "\n",
    "    def __next__(self):\n",
    "        print(\"Queue size: \", self.queue.qsize(), \"queued\", self.queued)\n",
    "        if not self.queue.full():\n",
    "            print(\"Prefetching\", self.prefetch_batches - self.queue.qsize(), \"batches\")\n",
    "            self.queued +=  self.prefetch_batches - self.queue.qsize()\n",
    "            futures = [self.executor.submit(self.prefetch) for _ in range(self.prefetch_batches - self.queue.qsize())]\n",
    "        return self.queue.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queue size:  0 queued 0\n",
      "Prefetching 10 batches\n",
      "Self.id:  0  -  56  -  35\n",
      "Self.id:  1  -  56  -  32\n",
      "Self.id:  2  -  56  -  7\n",
      "Self.id:  3  -  54  -  7\n",
      "Self.id:  4  -  54  -  30\n",
      "Self.id:  7  -  53  -  30\n",
      "Self.id:  5  -  54  -  3\n",
      "Self.id:  6  -  54  -  38\n",
      "Self.id:  8  -  53  -  44\n",
      "Self.id:  9  -  53  -  9\n",
      "['0']\n",
      "Queue size:  9 queued 0\n",
      "Prefetching 1 batches\n",
      "['2']\n",
      "Self.id:  0  -  57  -  40\n",
      "Queue size:  8 queued 1\n",
      "Prefetching 2 batches\n",
      "['1']\n",
      "Self.id:  1  -  57  -  13\n",
      "Self.id:  2  -  57  -  31\n",
      "Queue size:  7 queued 3\n",
      "Prefetching 3 batches\n",
      "['4']\n",
      "Self.id:  4  -  55  -  35\n",
      "Self.id:  5  -  55  -  42\n",
      "Self.id:  3  -  55  -  43\n",
      "Queue size:  7 queued 5\n",
      "Prefetching 3 batches\n",
      "['3']\n",
      "Self.id:  6  -  55  -  25\n",
      "Self.id:  8  -  54  -  41\n",
      "Self.id:  7  -  54  -  40\n",
      "Queue size:  8 queued 6\n",
      "Prefetching 2 batches\n",
      "['7']\n",
      "Self.id:  0  -  58  -  25\n",
      "Self.id:  9  -  54  -  14\n",
      "Queue size:  10 queued 5\n",
      "['5']\n",
      "Queue size:  10 queued 4\n",
      "['9']\n",
      "Queue size:  10 queued 3\n",
      "['8']\n",
      "Queue size:  10 queued 2\n",
      "['6']\n",
      "Queue size:  10 queued 1\n",
      "['0']\n",
      "Queue size:  10 queued 0\n",
      "['2']\n",
      "Queue size:  9 queued 0\n",
      "Prefetching 1 batches\n",
      "['1']\n",
      "Self.id:  1  -  58  -  44\n",
      "Queue size:  8 queued 1\n",
      "Prefetching 2 batches\n",
      "['5']\n",
      "Self.id:  2  -  58  -  14\n",
      "Self.id:  3  -  56  -  42\n",
      "Queue size:  7 queued 3\n",
      "Prefetching 3 batches\n",
      "['4']\n",
      "Self.id:  4  -  56  -  27\n",
      "Self.id:  5  -  56  -  32\n",
      "Self.id:  6  -  56  -  36\n",
      "Queue size:  7 queued 5\n",
      "Prefetching 3 batches\n",
      "['3']\n",
      "Self.id:  7  -  55  -  7\n",
      "Self.id:  8  -  55  -  5\n",
      "Self.id:  9  -  55  -  2\n",
      "Queue size:  8 queued 6\n",
      "Prefetching 2 batches\n",
      "['6']\n",
      "Self.id:  0  -  59  -  10\n",
      "Self.id:  1  -  59  -  34\n",
      "Queue size:  10 queued 5\n",
      "['8']\n",
      "Queue size:  10 queued 4\n",
      "['7']\n",
      "Queue size:  10 queued 3\n",
      "['9']\n",
      "Queue size:  10 queued 2\n",
      "['0']\n",
      "Queue size:  10 queued 1\n",
      "['1']\n",
      "Queue size:  10 queued 0\n",
      "['3']\n",
      "Queue size:  9 queued 0\n",
      "Prefetching 1 batches\n",
      "['2']\n",
      "Self.id:  2  -  59  -  47\n",
      "Queue size:  8 queued 1\n",
      "Prefetching 2 batches\n",
      "['6']\n",
      "Self.id:  3  -  57  -  48\n",
      "Self.id:  4  -  57  -  40\n",
      "Queue size:  7 queued 3\n",
      "Prefetching 3 batches\n",
      "['5']\n",
      "Self.id:  6  -  57  -  2\n",
      "Self.id:  7  -  56  -  48\n",
      "Self.id:  5  -  57  -  33\n",
      "Queue size:  7 queued 5\n",
      "Prefetching 3 batches\n",
      "['4']\n",
      "Self.id:  0  -  60  -  5\n",
      "Self.id:  8  -  56  -  19\n",
      "Self.id:  9  -  56  -  40\n",
      "Queue size:  8 queued 6\n",
      "Prefetching 2 batches\n",
      "['7']\n",
      "Self.id:  1  -  60  -  28\n",
      "Self.id:  2  -  60  -  14\n",
      "Queue size:  10 queued 5\n",
      "['8']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m my_loader:\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(data)\n\u001b[0;32m----> 5\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_loader = PrefetchLoader(datasets, batch_size=1, prefetch_batches=10)\n",
    "\n",
    "for data in my_loader:\n",
    "    print(data)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, num_workers=0, prefetch_factor=2):\n",
    "        super().__init__(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.prefetched_samples = []\n",
    "        self._original_iterator = iter(super().__iter__())\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.prefetched_samples = []\n",
    "        self._iterator = iter(self._original_iterator)\n",
    "        self._prefetch_samples()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self.prefetched_samples) == 0:\n",
    "            self._prefetch_samples()\n",
    "\n",
    "        if len(self.prefetched_samples) == 0:\n",
    "            raise StopIteration\n",
    "\n",
    "        return self.prefetched_samples.pop(0)\n",
    "\n",
    "    def _prefetch_samples(self):\n",
    "        while len(self.prefetched_samples) < self.prefetch_factor:\n",
    "            try:\n",
    "                batch = next(self._iterator)\n",
    "                self.prefetched_samples.append(batch)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self.id:  \n",
      "0Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n",
      "Self.id:  0\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the random dataset using the custom data loader\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "prefetch_factor = 2\n",
    "\n",
    "custom_loader = CustomDataLoader(random_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                                prefetch_factor=prefetch_factor)\n",
    "custom_iter = iter(custom_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting batch took 0.0005524158477783203 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.0003113746643066406 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.00818181037902832 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.00014162063598632812 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.8961060047149658 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.0002777576446533203 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.40130186080932617 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.0008094310760498047 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 1.6627304553985596 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.0006418228149414062 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.3868715763092041 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.0004820823669433594 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 1.631580114364624 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.00044226646423339844 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.38702845573425293 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "Getting batch took 0.0003788471221923828 seconds\n",
      "2\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m time1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(custom_iter)\n\u001b[1;32m      5\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m, in \u001b[0;36mCustomDataLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetched_samples) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prefetch_samples()\n\u001b[1;32m     21\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetched_samples) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     22\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 29\u001b[0m, in \u001b[0;36mCustomDataLoader._prefetch_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetched_samples) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_factor:\n\u001b[1;32m     28\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[1;32m     30\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetched_samples\u001b[39m.\u001b[39mappend(batch)\n\u001b[1;32m     31\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    108\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/data/vision/polina/projects/wmh/dhollidt/conda/envs/nerfstudio2/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    time1 = time.time()\n",
    "    try:\n",
    "        batch = next(custom_iter)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    time2 = time.time()\n",
    "    print(\"Getting batch took {} seconds\".format(time2-time1))\n",
    "    print(len(batch))\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    # Use the batch for training or processing\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultiDataLoader(DataLoader):\n",
    "    def __init__(self, datasets, batch_size=1, shuffle=False, num_workers=0, prefetch_factor=2):\n",
    "        super().__init__(datasets[0], batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.prefetched_samples = []\n",
    "        self._datasets = datasets\n",
    "        self._original_iterators = [iter(dataset) for dataset in self._datasets]\n",
    "        self._current_dataset_index = 0\n",
    "        self._original_iterator = self._original_iterators[self._current_dataset_index]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.prefetched_samples = []\n",
    "        self._current_dataset_index = 0\n",
    "        self._original_iterator = self._original_iterators[self._current_dataset_index]\n",
    "        self._prefetch_samples()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self.prefetched_samples) == 0:\n",
    "            self._prefetch_samples()\n",
    "\n",
    "        if len(self.prefetched_samples) == 0:\n",
    "            raise StopIteration\n",
    "\n",
    "        return self.prefetched_samples.pop(0)\n",
    "\n",
    "    def _prefetch_samples(self):\n",
    "        while len(self.prefetched_samples) < self.prefetch_factor:\n",
    "            try:\n",
    "                self._current_dataset_index = (self._current_dataset_index + 1) % len(self._datasets)\n",
    "                self._original_iterator = self._original_iterators[self._current_dataset_index]\n",
    "                batch = next(self._original_iterator)\n",
    "                self.prefetched_samples.append(batch)\n",
    "            except StopIteration:\n",
    "                print(\"StopIteration\")\n",
    "                self._current_dataset_index = (self._current_dataset_index + 1) % len(self._datasets)\n",
    "                self._original_iterator = self._original_iterators[self._current_dataset_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self.id:  1\n",
      "Self.id:  2\n",
      "Getting batch took 4.982948303222656e-05 seconds\n",
      "2\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([10])\n",
      "Getting batch took 2.6464462280273438e-05 seconds\n",
      "2\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([10])\n",
      "Self.id:  3\n",
      "Self.id:  4\n",
      "Getting batch took 4.0117106437683105 seconds\n",
      "2\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([10])\n",
      "Getting batch took 0.0005445480346679688 seconds\n",
      "2\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([10])\n",
      "Self.id:  5\n",
      "Self.id:  6\n",
      "Getting batch took 4.008026361465454 seconds\n",
      "2\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([10])\n",
      "Getting batch took 4.315376281738281e-05 seconds\n",
      "2\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([10])\n",
      "Self.id:  7\n",
      "Self.id:  8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m time1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(multi_iter)\n\u001b[1;32m     10\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 20\u001b[0m, in \u001b[0;36mCustomMultiDataLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetched_samples) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prefetch_samples()\n\u001b[1;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetched_samples) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 32\u001b[0m, in \u001b[0;36mCustomMultiDataLoader._prefetch_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_dataset_index \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_dataset_index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_datasets)\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterators[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_dataset_index]\n\u001b[0;32m---> 32\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_original_iterator)\n\u001b[1;32m     33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetched_samples\u001b[39m.\u001b[39mappend(batch)\n\u001b[1;32m     34\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m, in \u001b[0;36mRandomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSelf.id: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid)\n\u001b[1;32m     13\u001b[0m \u001b[39m# print(\"Get item called\")\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m input_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size)\n\u001b[1;32m     16\u001b[0m target_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_workers = 2\n",
    "prefetch_factor = 2\n",
    "custom_loader = CustomMultiDataLoader(datasets, batch_size=batch_size, prefetch_factor=prefetch_factor, shuffle=True, num_workers=num_workers)\n",
    "multi_iter = iter(custom_loader)\n",
    "while True:\n",
    "    time1 = time.time()\n",
    "    try:\n",
    "        batch = next(multi_iter)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    time2 = time.time()\n",
    "    print(\"Getting batch took {} seconds\".format(time2-time1))\n",
    "    print(len(batch))\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    # Use the batch for training or processing\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path_0 = \"/data/vision/polina/projects/wmh/dhollidt/documents/nerf/klever_depth_normal_models_nesf/0/depth-nerfacto/07_04_23/nerfstudio_models/step-000007999.ckpt\"\n",
    "load_path_1 = \"/data/vision/polina/projects/wmh/dhollidt/documents/nerf/klever_depth_normal_models_nesf/1/depth-nerfacto/07_04_23/nerfstudio_models/step-000007999.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(load_path):\n",
    "    time1 = time.time()\n",
    "    loaded_state = torch.load(load_path, map_location=\"cpu\")[\"pipeline\"]\n",
    "    time2 = time.time()\n",
    "\n",
    "    model=NerfactoModelConfig(eval_num_rays_per_chunk=1 << 15,\n",
    "                                        predict_normals=True\n",
    "                                        )\n",
    "    time_2_5 = time.time()\n",
    "    scene_box = SceneBox(aabb = torch.zeros((2,3)))\n",
    "    model = model.setup(scene_box=scene_box,\n",
    "                num_train_data=271,\n",
    "                metadata={})\n",
    "\n",
    "    time3 = time.time()\n",
    "    state = {key.replace(\"module.\", \"\"): value for key, value in loaded_state.items()}\n",
    "    state = {key.replace(\"_model.\", \"\"): value for key, value in state.items()}\n",
    "    time4 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(state, strict=False)\n",
    "    # gt_sd = model.state_dict()\n",
    "    print(\"Missing keys: {}\".format(missing_keys))\n",
    "    print(\"Unexpected keys: {}\".format(unexpected_keys))\n",
    "    time5 = time.time()\n",
    "\n",
    "    print(\"Loading took {} seconds\".format(time2-time1))\n",
    "    print(\"Creating config took {} seconds\".format(time_2_5-time2))\n",
    "    print(\"Model setup took {} seconds\".format(time3-time_2_5))\n",
    "    print(\"State dict conversion took {} seconds\".format(time4-time3))\n",
    "    print(\"Loading state dict took {} seconds\".format(time5-time4))\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    time1 = time.time()\n",
    "    model=NerfactoModelConfig(eval_num_rays_per_chunk=1 << 15,\n",
    "                                        predict_normals=True\n",
    "                                        )\n",
    "    time_2 = time.time()\n",
    "    scene_box = SceneBox(aabb = torch.zeros((2,3)))\n",
    "    model = model.setup(scene_box=scene_box,\n",
    "                num_train_data=271,\n",
    "                metadata={})\n",
    "    time3 = time.time()\n",
    "    print(\"Creating config took {} seconds\".format(time_2-time1))\n",
    "    print(\"Model setup took {} seconds\".format(time3-time_2))\n",
    "    return model\n",
    "\n",
    "def get_state_dicts(count):\n",
    "    state_dicts = []\n",
    "    for i in range(count):\n",
    "        time1 = time.time()\n",
    "        load_path = \"/data/vision/polina/projects/wmh/dhollidt/documents/nerf/klever_depth_normal_models_nesf/{}/depth-nerfacto/07_04_23/nerfstudio_models/step-000007999.ckpt\".format(i)\n",
    "        loaded_state = torch.load(load_path, map_location=\"cpu\")[\"pipeline\"]\n",
    "        state = {key.replace(\"module.\", \"\"): value for key, value in loaded_state.items()}\n",
    "        state = {key.replace(\"_model.\", \"\"): value for key, value in state.items()}\n",
    "        state_dicts.append(state)\n",
    "        time2 = time.time()\n",
    "        print(\"Loading took {} seconds\".format(time2-time1))\n",
    "    return state_dicts\n",
    "\n",
    "def load_state_dict_to_model(model, state_dict):\n",
    "    time3 = time.time()\n",
    "    state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "    state_dict = {key.replace(\"_model.\", \"\"): value for key, value in state_dict.items()}\n",
    "    time4 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "    # gt_sd = model.state_dict()\n",
    "    print(\"Missing keys: {}\".format(missing_keys))\n",
    "    print(\"Unexpected keys: {}\".format(unexpected_keys))\n",
    "    time5 = time.time()\n",
    "\n",
    "    print(\"State dict conversion took {} seconds\".format(time4-time3))\n",
    "    print(\"Loading state dict took {} seconds\".format(time5-time4))\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading took 0.5905206203460693 seconds\n",
      "Loading took 0.1249542236328125 seconds\n",
      "Loading took 0.5644783973693848 seconds\n",
      "Loading took 0.5164320468902588 seconds\n",
      "Loading took 0.5863499641418457 seconds\n",
      "Loading took 0.4617340564727783 seconds\n",
      "Loading took 0.49069833755493164 seconds\n",
      "Loading took 0.48426032066345215 seconds\n",
      "Loading took 0.5144634246826172 seconds\n",
      "Loading took 0.4097611904144287 seconds\n"
     ]
    }
   ],
   "source": [
    "state_dicts = get_state_dicts(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating config took 5.5789947509765625e-05 seconds\n",
      "Model setup took 0.9349899291992188 seconds\n",
      "Creating config took 4.673004150390625e-05 seconds\n",
      "Model setup took 0.7964234352111816 seconds\n",
      "Creating config took 2.5033950805664062e-05 seconds\n",
      "Model setup took 0.7067139148712158 seconds\n",
      "Creating config took 1.8358230590820312e-05 seconds\n",
      "Model setup took 0.8017966747283936 seconds\n",
      "Creating config took 2.288818359375e-05 seconds\n",
      "Model setup took 0.7316091060638428 seconds\n",
      "Creating config took 1.8358230590820312e-05 seconds\n",
      "Model setup took 0.8432517051696777 seconds\n",
      "Creating config took 2.3603439331054688e-05 seconds\n",
      "Model setup took 0.7980501651763916 seconds\n",
      "Creating config took 1.8358230590820312e-05 seconds\n",
      "Model setup took 0.7507121562957764 seconds\n",
      "Creating config took 1.7404556274414062e-05 seconds\n",
      "Model setup took 0.87148118019104 seconds\n",
      "Creating config took 1.8358230590820312e-05 seconds\n",
      "Model setup took 0.8608250617980957 seconds\n",
      "Creating config took 2.8848648071289062e-05 seconds\n",
      "Model setup took 0.9105930328369141 seconds\n",
      "Creating config took 3.266334533691406e-05 seconds\n",
      "Model setup took 0.8566782474517822 seconds\n",
      "Creating config took 1.7404556274414062e-05 seconds\n",
      "Model setup took 0.7631878852844238 seconds\n",
      "Creating config took 2.288818359375e-05 seconds\n",
      "Model setup took 0.8089010715484619 seconds\n",
      "Creating config took 1.7881393432617188e-05 seconds\n",
      "Model setup took 0.8290205001831055 seconds\n",
      "Creating config took 1.7881393432617188e-05 seconds\n",
      "Model setup took 0.7784676551818848 seconds\n",
      "Creating config took 1.811981201171875e-05 seconds\n",
      "Model setup took 0.78987717628479 seconds\n",
      "Creating config took 2.7418136596679688e-05 seconds\n",
      "Model setup took 0.8318541049957275 seconds\n",
      "Creating config took 1.7404556274414062e-05 seconds\n",
      "Model setup took 0.7530419826507568 seconds\n",
      "Creating config took 2.0742416381835938e-05 seconds\n",
      "Model setup took 0.8915457725524902 seconds\n",
      "Creating config took 2.3126602172851562e-05 seconds\n",
      "Model setup took 0.9264411926269531 seconds\n",
      "Creating config took 2.4080276489257812e-05 seconds\n",
      "Model setup took 0.8289813995361328 seconds\n",
      "Creating config took 5.1975250244140625e-05 seconds\n",
      "Model setup took 0.9173116683959961 seconds\n",
      "Creating config took 2.956390380859375e-05 seconds\n",
      "Model setup took 0.789578914642334 seconds\n",
      "Creating config took 2.3126602172851562e-05 seconds\n",
      "Model setup took 0.9271948337554932 seconds\n",
      "Creating config took 4.982948303222656e-05 seconds\n",
      "Model setup took 0.9338011741638184 seconds\n",
      "Creating config took 1.9073486328125e-05 seconds\n",
      "Model setup took 0.7686560153961182 seconds\n",
      "Creating config took 2.86102294921875e-05 seconds\n",
      "Model setup took 0.7312092781066895 seconds\n",
      "Creating config took 1.71661376953125e-05 seconds\n",
      "Model setup took 0.8659670352935791 seconds\n",
      "Creating config took 2.288818359375e-05 seconds\n",
      "Model setup took 0.762913703918457 seconds\n",
      "Creating config took 1.6450881958007812e-05 seconds\n",
      "Model setup took 1.014547348022461 seconds\n",
      "Creating config took 1.7404556274414062e-05 seconds\n",
      "Model setup took 0.8297460079193115 seconds\n",
      "Creating config took 9.846687316894531e-05 seconds\n",
      "Model setup took 0.8361551761627197 seconds\n",
      "Creating config took 2.1219253540039062e-05 seconds\n",
      "Model setup took 0.816967248916626 seconds\n",
      "Creating config took 1.8596649169921875e-05 seconds\n",
      "Model setup took 0.8238492012023926 seconds\n",
      "Creating config took 2.384185791015625e-05 seconds\n",
      "Model setup took 0.7894251346588135 seconds\n",
      "Creating config took 1.811981201171875e-05 seconds\n",
      "Model setup took 0.7656266689300537 seconds\n",
      "Creating config took 1.6927719116210938e-05 seconds\n",
      "Model setup took 0.7679574489593506 seconds\n",
      "Creating config took 1.8596649169921875e-05 seconds\n",
      "Model setup took 0.7317521572113037 seconds\n",
      "Creating config took 2.574920654296875e-05 seconds\n",
      "Model setup took 0.6883094310760498 seconds\n",
      "Creating config took 2.5033950805664062e-05 seconds\n",
      "Model setup took 0.677757978439331 seconds\n",
      "Creating config took 2.574920654296875e-05 seconds\n",
      "Model setup took 0.7660140991210938 seconds\n",
      "Creating config took 2.5272369384765625e-05 seconds\n",
      "Model setup took 0.820286750793457 seconds\n",
      "Creating config took 2.5272369384765625e-05 seconds\n",
      "Model setup took 0.8461222648620605 seconds\n",
      "Creating config took 1.8358230590820312e-05 seconds\n",
      "Model setup took 0.7486903667449951 seconds\n",
      "Creating config took 4.3392181396484375e-05 seconds\n",
      "Model setup took 0.7671017646789551 seconds\n",
      "Creating config took 1.7881393432617188e-05 seconds\n",
      "Model setup took 0.75461745262146 seconds\n",
      "Creating config took 2.3365020751953125e-05 seconds\n",
      "Model setup took 0.6813826560974121 seconds\n",
      "Creating config took 1.811981201171875e-05 seconds\n",
      "Model setup took 0.7514593601226807 seconds\n",
      "Creating config took 1.811981201171875e-05 seconds\n",
      "Model setup took 0.8130109310150146 seconds\n",
      "Creating config took 1.9550323486328125e-05 seconds\n",
      "Model setup took 0.7579803466796875 seconds\n",
      "Average model loading time: 0.8056656217575073\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "# compute average model loading time\n",
    "time1 = time.time()\n",
    "K=50\n",
    "for i in range(K):\n",
    "    model = get_model()\n",
    "time2 = time.time()\n",
    "print(\"Average model loading time: {}\".format((time2-time1)/K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 1.9073486328125e-05 seconds\n",
      "Loading state dict took 0.022670745849609375 seconds\n",
      "cpu\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 2.7179718017578125e-05 seconds\n",
      "Loading state dict took 0.02508378028869629 seconds\n",
      "cpu\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 2.4318695068359375e-05 seconds\n",
      "Loading state dict took 0.030529260635375977 seconds\n",
      "cpu\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 2.3603439331054688e-05 seconds\n",
      "Loading state dict took 0.03043675422668457 seconds\n",
      "cpu\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 2.3126602172851562e-05 seconds\n",
      "Loading state dict took 0.03284716606140137 seconds\n",
      "cpu\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 3.5762786865234375e-05 seconds\n",
      "Loading state dict took 0.03047919273376465 seconds\n",
      "cpu\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 2.47955322265625e-05 seconds\n",
      "Loading state dict took 0.03367424011230469 seconds\n",
      "cpu\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 2.7179718017578125e-05 seconds\n",
      "Loading state dict took 0.032102108001708984 seconds\n",
      "cpu\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 2.5987625122070312e-05 seconds\n",
      "Loading state dict took 0.17356228828430176 seconds\n",
      "cpu\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "State dict conversion took 3.0994415283203125e-05 seconds\n",
      "Loading state dict took 0.11500263214111328 seconds\n",
      "cpu\n",
      "Average model loading time: 0.05275692939758301\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "for state_dict in state_dicts:\n",
    "    model = load_state_dict_to_model(model, state_dict)\n",
    "    print(model.device)\n",
    "    \n",
    "time2 = time.time()\n",
    "print(\"Average model loading time: {}\".format((time2-time1)/len(state_dicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "Loading took 0.661447286605835 seconds\n",
      "Creating config took 7.367134094238281e-05 seconds\n",
      "Model setup took 10.829570770263672 seconds\n",
      "State dict conversion took 8.726119995117188e-05 seconds\n",
      "Loading state dict took 0.264911413192749 seconds\n",
      "SceneBox(aabb=tensor([[-1., -1., -1.],\n",
      "        [ 1.,  1.,  1.]]))\n"
     ]
    }
   ],
   "source": [
    "m0 = load_model(load_path_0)\n",
    "print(m0.scene_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "Loading took 0.878633975982666 seconds\n",
      "Creating config took 4.673004150390625e-05 seconds\n",
      "Model setup took 2.973586082458496 seconds\n",
      "State dict conversion took 3.1948089599609375e-05 seconds\n",
      "Loading state dict took 0.02256464958190918 seconds\n",
      "SceneBox(aabb=tensor([[-1., -1., -1.],\n",
      "        [ 1.,  1.,  1.]]))\n"
     ]
    }
   ],
   "source": [
    "m1 = load_model(load_path_1)\n",
    "print(m1.scene_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'UniformLinDispPiecewiseSampler.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m time1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m pickle\u001b[39m.\u001b[39;49mdump(m0, \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mm0.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m time2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m \u001b[39m# load the model from disk\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'UniformLinDispPiecewiseSampler.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "time1 = time.time()\n",
    "pickle.dump(m0, open(\"m0.pkl\", \"wb\"))\n",
    "time2 = time.time()\n",
    "\n",
    "# load the model from disk\n",
    "model = pickle.load(open(\"m0.pkl\", 'rb'))\n",
    "time3 = time.time()\n",
    "print(\"Pickling took {} seconds\".format(time2-time1))\n",
    "print(\"Unpickling took {} seconds\".format(time3-time2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
