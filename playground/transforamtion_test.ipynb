{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config ZMQInteractiveShell.cache_size = 0\n",
    "\n",
    "# import all the necessary modules\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from nerfstudio.models.nerfacto import NerfactoModelConfig\n",
    "from nerfstudio.configs.base_config import ViewerConfig\n",
    "from nerfstudio.configs.experiment_config import ExperimentConfig\n",
    "from nerfstudio.pipelines.base_pipeline import VanillaPipelineConfig\n",
    "from nerfstudio.data.datamanagers.base_datamanager import VanillaDataManagerConfig\n",
    "from nerfstudio.data.dataparsers.nerfstudio_dataparser import NerfstudioDataParserConfig\n",
    "from nerfstudio.cameras.camera_optimizers import CameraOptimizerConfig\n",
    "from nerfstudio.engine.optimizers import AdamOptimizerConfig, RAdamOptimizerConfig\n",
    "from nerfstudio.engine.trainer import TrainerConfig\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12:34:21] </span>Saving config to:                                                                    <a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/configs/experiment_config.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment_config.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/configs/experiment_config.py#124\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/data/vision/polina/projects/wmh/dhollidt/documents/nerf/playground/tmp/nerfacto/202</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                        </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">3-02-26_123421/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">config.yml</span>                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                        </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12:34:21]\u001b[0m\u001b[2;36m \u001b[0mSaving config to:                                                                    \u001b]8;id=610525;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/configs/experiment_config.py\u001b\\\u001b[2mexperiment_config.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=195409;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/configs/experiment_config.py#124\u001b\\\u001b[2m124\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[35m/data/vision/polina/projects/wmh/dhollidt/documents/nerf/playground/tmp/nerfacto/202\u001b[0m \u001b[2m                        \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[35m3-02-26_123421/\u001b[0m\u001b[95mconfig.yml\u001b[0m                                                            \u001b[2m                        \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12:34:21] </span>Saving checkpoints to:                                                                         <a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/engine/trainer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">trainer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/engine/trainer.py#122\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/data/vision/polina/projects/wmh/dhollidt/documents/nerf/playground/tmp/nerfacto/2023-02-26_12</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">3421/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">nerfstudio_models</span>                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12:34:21]\u001b[0m\u001b[2;36m \u001b[0mSaving checkpoints to:                                                                         \u001b]8;id=424125;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/engine/trainer.py\u001b\\\u001b[2mtrainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=720065;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/engine/trainer.py#122\u001b\\\u001b[2m122\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[35m/data/vision/polina/projects/wmh/dhollidt/documents/nerf/playground/tmp/nerfacto/2023-02-26_12\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[35m3421/\u001b[0m\u001b[95mnerfstudio_models\u001b[0m                                                                         \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdominik-hollidt\u001b[0m (\u001b[33mdhollidt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/vision/polina/projects/wmh/dhollidt/documents/nerf/playground/tmp/nerfacto/2023-02-26_123421/wandb/run-20230226_123428-bd3w1f6o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dhollidt/nerfstudio-project/runs/bd3w1f6o' target=\"_blank\">colorful-dust-186</a></strong> to <a href='https://wandb.ai/dhollidt/nerfstudio-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dhollidt/nerfstudio-project' target=\"_blank\">https://wandb.ai/dhollidt/nerfstudio-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dhollidt/nerfstudio-project/runs/bd3w1f6o' target=\"_blank\">https://wandb.ai/dhollidt/nerfstudio-project/runs/bd3w1f6o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">logging events to: /data/vision/polina/projects/wmh/dhollidt/documents/nerf/playground/tmp/nerfacto/2023-02-26_123421</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mlogging events to: \u001b[0m\u001b[1;33m/data/vision/polina/projects/wmh/dhollidt/documents/nerf/playground/tmp/nerfacto/\u001b[0m\u001b[1;33m2023-02-26_123421\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12:34:33] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                                                 <a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#314\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">314</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12:34:33]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m1\u001b[0m                                                 \u001b]8;id=128354;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=139327;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#314\u001b\\\u001b[2m314\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Skipping <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> files in dataset split train.                                         <a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split train.                                         \u001b]8;id=808516;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=659131;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Skipping <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> files in dataset split val.                                           <a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split val.                                           \u001b]8;id=724220;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=829723;file:///data/vision/polina/projects/wmh/dhollidt/documents/nerf/nerfstudio_fork/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">232</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m232\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd58397d1c4420cb37b5e5c80495a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up evaluation dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m25\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731e3e6464c0451f9a5af639b9329dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">done loading checkpoint from \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/data/vision/polina/projects/wmh/dhollidt/documents/nerf/outputs/dice_256/nerfacto/2023-01-16_101826/nerfstudio_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">s</span>\n",
       "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tep-000024000.ckpt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "done loading checkpoint from \n",
       "\u001b[35m/data/vision/polina/projects/wmh/dhollidt/documents/nerf/outputs/dice_256/nerfacto/2023-01-16_101826/nerfstudio_models/\u001b[0m\u001b[95ms\u001b[0m\n",
       "\u001b[95mtep-000024000.ckpt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dice DATA\n",
    "MODEL_CHECKPOINT_PATH = Path(\"/data/vision/polina/projects/wmh/dhollidt/documents/nerf/outputs/dice_256/nerfacto/2023-01-16_101826/nerfstudio_models\")\n",
    "MODEL_LOAD_STEP = 24000\n",
    "DATA_PATH = Path(\"/data/vision/polina/projects/wmh/dhollidt/documents/nerf/data/dice_rand_v3\")\n",
    "\n",
    "# CLEVR DATA\n",
    "# MODEL_CHECKPOINT_PATH = Path(\"/data/vision/polina/scratch/clintonw/datasets/nerfacto/0/nerfacto/2023-01-13_145424/nerfstudio_models\")\n",
    "# MODEL_LOAD_STEP = 29999\n",
    "# DATA_PATH = Path(\"/data/vision/polina/scratch/clintonw/datasets/kubric/0/\")\n",
    "\n",
    "OUTPUT_DIR = Path(\"/data/vision/polina/projects/wmh/dhollidt/documents/nerf/playground\")\n",
    "\n",
    "\n",
    "trainConfig = TrainerConfig(\n",
    "    method_name=\"nerfacto\",\n",
    "    experiment_name=\"/tmp\",\n",
    "    data=DATA_PATH,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    steps_per_eval_batch=500,\n",
    "    steps_per_save=2000,\n",
    "    max_num_iterations=30000,\n",
    "    mixed_precision=True,\n",
    "    pipeline=VanillaPipelineConfig(\n",
    "        datamanager=VanillaDataManagerConfig(\n",
    "            dataparser=NerfstudioDataParserConfig(),\n",
    "            train_num_rays_per_batch=4096,\n",
    "            eval_num_rays_per_batch=4096,\n",
    "            camera_optimizer=CameraOptimizerConfig(\n",
    "                mode=\"off\", optimizer=AdamOptimizerConfig(lr=6e-4, eps=1e-8, weight_decay=1e-2)\n",
    "            ),\n",
    "        ),\n",
    "        model=NerfactoModelConfig(eval_num_rays_per_chunk=1 << 15),\n",
    "    ),\n",
    "    optimizers={\n",
    "        \"proposal_networks\": {\n",
    "            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n",
    "            \"scheduler\": None,\n",
    "        },\n",
    "        \"fields\": {\n",
    "            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n",
    "            \"scheduler\": None,\n",
    "        },\n",
    "    },\n",
    "    viewer=ViewerConfig(num_rays_per_chunk=1 << 15),\n",
    "    vis=\"wandb\",\n",
    "    load_dir=MODEL_CHECKPOINT_PATH,\n",
    "    load_step=MODEL_LOAD_STEP\n",
    ")\n",
    "\n",
    "trainConfig.set_timestamp()\n",
    "trainConfig.pipeline.datamanager.dataparser.data = trainConfig.data\n",
    "trainConfig.save_config()\n",
    "\n",
    "trainer = trainConfig.setup(local_rank=0, world_size=1)\n",
    "trainer.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network has 13,633,168 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "pipeline = trainer.pipeline\n",
    "model = pipeline.model\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The network has {num_params:,} trainable parameters.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "ray_bundle, batch = pipeline.datamanager.next_train(1)\n",
    "ray_bundle_eval, batch_eval = pipeline.datamanager.next_eval(1)\n",
    "print(ray_bundle.shape)\n",
    "print(ray_bundle_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray_bundle.origins.shape=torch.Size([4096, 3])\n",
      "ray_bundle.directions.shape=torch.Size([4096, 3])\n",
      "ray_samples.frustums.starts.shape=torch.Size([4096, 48, 1])\n",
      "ray_samples.frustums.ends.shape=torch.Size([4096, 48, 1])\n",
      "ray_samples.frustums.get_positions().shape=torch.Size([4096, 48, 3])\n",
      "ray_samples.shape=torch.Size([4096, 48])\n",
      "torch.Size([4096, 48, 3])\n",
      "torch.Size([4096, 48, 1])\n",
      "dict_keys([<FieldHeadNames.RGB: 'rgb'>, <FieldHeadNames.DENSITY: 'density'>])\n"
     ]
    }
   ],
   "source": [
    "if model.collider is not None:\n",
    "    ray_bundle = model.collider(ray_bundle)\n",
    "\n",
    "ray_samples, weights_list, ray_samples_list = model.proposal_sampler(ray_bundle, density_fns=model.density_fns)\n",
    "field_outputs = model.field(ray_samples, compute_normals=model.config.predict_normals)\n",
    "\n",
    "print(f\"{ray_bundle.origins.shape=}\")\n",
    "print(f\"{ray_bundle.directions.shape=}\")\n",
    "print(f\"{ray_samples.frustums.starts.shape=}\")\n",
    "print(f\"{ray_samples.frustums.ends.shape=}\")\n",
    "print(f\"{ray_samples.frustums.get_positions().shape=}\")\n",
    "print(f\"{ray_samples.shape=}\")\n",
    "\n",
    "print(field_outputs[FieldHeadNames.RGB].shape)\n",
    "print(field_outputs[FieldHeadNames.DENSITY].shape)\n",
    "print(field_outputs.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([4096, 3])\n",
    "torch.Size([4096, 3])\n",
    "torch.Size([4096, 48, 1])\n",
    "torch.Size([4096, 48, 1])\n",
    "torch.Size([4096, 48, 3])\n",
    "torch.Size([4096, 48, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tinycudann as tcnn\n",
    "except ImportError:\n",
    "    # tinycudann module doesn't exist\n",
    "    pass\n",
    "from torch import Tensor, nn\n",
    "from nerfstudio.models.base_model import Model, ModelConfig\n",
    "from nerfstudio.models.nerfacto import NerfactoModel\n",
    "from typing import cast\n",
    "from nerfstudio.fields.nerfacto_field import get_normalized_directions\n",
    "class FeatureGenerator(nn.Module):\n",
    "    \"\"\"Takes in a batch of b Ray bundles, samples s points along the ray. Then it outputs n x m x f matrix.\n",
    "    Each row corresponds to one feature of a sampled point of the ray.\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, positional_encoding_dim=128, field_output_encoding=128):\n",
    "        super().__init__()\n",
    "        self.direction_encoding = tcnn.Encoding(\n",
    "            n_input_dims=3,\n",
    "            encoding_config={\n",
    "                \"otype\": \"SphericalHarmonics\",\n",
    "                \"degree\": 4,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        self.position_frustums_encoding = tcnn.Encoding(\n",
    "            n_input_dims=3,\n",
    "            encoding_config={\"otype\": \"Frequency\", \"n_frequencies\": 2},\n",
    "        )\n",
    "\n",
    "        self.mlp_merged_pos_encoding = tcnn.Network(\n",
    "            n_input_dims=self.direction_encoding.n_output_dims + self.position_frustums_encoding.n_output_dims,\n",
    "            n_output_dims=positional_encoding_dim,\n",
    "            network_config={\n",
    "                \"otype\": \"FullyFusedMLP\",\n",
    "                \"activation\": \"ReLU\",\n",
    "                \"output_activation\": \"None\",\n",
    "                \"n_neurons\": 64,\n",
    "                \"n_hidden_layers\": 4,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Tiny cudnn network for processing the field outputs of the samples b x s x (rgb + density = 4)\n",
    "        self.mlp_field_output = tcnn.Network(\n",
    "            n_input_dims=4,\n",
    "            n_output_dims=field_output_encoding,\n",
    "            network_config={\n",
    "                \"otype\": \"FullyFusedMLP\",\n",
    "                \"activation\": \"ReLU\",\n",
    "                \"output_activation\": \"None\",\n",
    "                \"n_neurons\": 64,\n",
    "                \"n_hidden_layers\": 4,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def forward(self, ray_bundle: RayBundle, model: Model) -> Tensor:\n",
    "\n",
    "        if isinstance(model, NerfactoModel):\n",
    "            model = cast(NerfactoModel, model)\n",
    "            if model.collider is not None:\n",
    "                ray_bundle = model.collider(ray_bundle)\n",
    "\n",
    "            ray_samples, _, _ = model.proposal_sampler(ray_bundle, density_fns=model.density_fns)\n",
    "            field_outputs = model.field(ray_samples, compute_normals=model.config.predict_normals)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only NerfactoModel is supported for now\")\n",
    "\n",
    "\n",
    "        # normalize field densitities:\n",
    "        densities = field_outputs[FieldHeadNames.DENSITY]\n",
    "        mean_vals = torch.mean(densities, dim=1, keepdim=True)\n",
    "        std_vals = torch.std(densities, dim=1, keepdim=True)\n",
    "\n",
    "        # Scale the values to have a mean of 0 and a standard deviation of 1\n",
    "        normalized_densities = (densities - mean_vals) / std_vals\n",
    "\n",
    "        field_outputs_stacked = torch.cat((field_outputs[FieldHeadNames.RGB], normalized_densities), dim=-1)\n",
    "        print(f\"{torch.isnan(field_outputs[FieldHeadNames.DENSITY]).sum()=}, \\n{field_outputs[FieldHeadNames.DENSITY].shape=} \")\n",
    "        print(f\"{normalized_densities.max()=}, {normalized_densities.min()=}, {normalized_densities.mean()=}, {normalized_densities.std()=}\")\n",
    "        print(f\"{torch.isnan(field_outputs_stacked).sum()=}\")\n",
    "        field_features = self.mlp_field_output(field_outputs_stacked.view(-1, 4))\n",
    "        print(f\"{torch.isnan(field_features).sum()=}\")\n",
    "        \n",
    "\n",
    "        # Positional encoding of the Frustums\n",
    "        positions_frustums = ray_samples.frustums.get_positions()\n",
    "        positions_frustums_flat = self.position_frustums_encoding(positions_frustums.view(-1, 3))\n",
    "        print(f\"{torch.isnan(positions_frustums_flat).sum()=}\")\n",
    "\n",
    "        # Positional encoding of the ray\n",
    "        directions = get_normalized_directions(ray_samples.frustums.directions)\n",
    "        directions_flat = directions.view(-1, 3)\n",
    "        d = self.direction_encoding(directions_flat)\n",
    "        print(f\"{torch.isnan(d).sum()=}\")\n",
    "        \n",
    "\n",
    "        pos_encode = torch.cat([d, positions_frustums_flat], dim=1)\n",
    "        \n",
    "        pos_features = self.mlp_merged_pos_encoding(pos_encode)\n",
    "        print(f\"{torch.isnan(pos_features).sum()=}\")\n",
    "        \n",
    "        \n",
    "        features = torch.cat([pos_features, field_features], dim=1)\n",
    "        features = features.view(ray_samples.shape[0], ray_samples.shape[1], -1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FeatureGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fg \u001b[39m=\u001b[39m FeatureGenerator()\n\u001b[1;32m      3\u001b[0m num_params \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m fg\u001b[39m.\u001b[39mparameters() \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe network has \u001b[39m\u001b[39m{\u001b[39;00mnum_params\u001b[39m:\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m trainable parameters.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FeatureGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "fg = FeatureGenerator()\n",
    "\n",
    "num_params = sum(p.numel() for p in fg.parameters() if p.requires_grad)\n",
    "print(f\"The network has {num_params:,} trainable parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.isnan(field_outputs[FieldHeadNames.DENSITY]).sum()=tensor(0, device='cuda:0'), \n",
      "field_outputs[FieldHeadNames.DENSITY].shape=torch.Size([256, 48, 1]) \n",
      "normalized_densities.max()=tensor(6.7839, device='cuda:0', grad_fn=<MaxBackward1>), normalized_densities.min()=tensor(-0.6007, device='cuda:0', grad_fn=<MinBackward1>), normalized_densities.mean()=tensor(-6.8297e-09, device='cuda:0', grad_fn=<MeanBackward0>), normalized_densities.std()=tensor(0.9896, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "torch.isnan(field_outputs_stacked).sum()=tensor(0, device='cuda:0')\n",
      "torch.isnan(field_features).sum()=tensor(0, device='cuda:0')\n",
      "torch.isnan(positions_frustums_flat).sum()=tensor(0, device='cuda:0')\n",
      "torch.isnan(d).sum()=tensor(0, device='cuda:0')\n",
      "torch.isnan(pos_features).sum()=tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "features = fg(ray_bundle, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.isnan(features).sum()=tensor(0, device='cuda:0')\n",
      "features.shape=torch.Size([256, 48, 256])\n"
     ]
    }
   ],
   "source": [
    "# count the nan in features\n",
    "print(f\"{torch.isnan(features).sum()=}\")\n",
    "print(f\"{features.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(1,64,128,256)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(256, 128, 64, 1)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(1, 16, 32, 64), dec_chs=(64, 32, 16), num_class=4):\n",
    "        super().__init__()\n",
    "        self.encoder            = Encoder(enc_chs)\n",
    "        self.decoder            = Decoder(dec_chs)\n",
    "        self.head               = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.density_activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        \n",
    "        # reduce the channel dimension\n",
    "        out      = torch.mean(out, dim=-1)\n",
    "        \n",
    "        output = {}\n",
    "        output[FieldHeadNames.RGB] = torch.sigmoid(out[:, :3, :].squeeze(1).permute(0, 2, 1))\n",
    "        output[FieldHeadNames.DENSITY] = self.density_activation(out[:, 3:, :].permute(0, 2, 1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network has 117,444 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "unet = UNet()\n",
    "unet = unet.half()\n",
    "unet = unet.to(\"cuda\")\n",
    "\n",
    "num_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "print(f\"The network has {num_params:,} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 48, 256])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape\n",
    "f = features.unsqueeze(1)\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 16, 48, 256])\n",
      "torch.Size([256, 4, 48, 256])\n",
      "torch.Size([256, 4, 48])\n",
      "out[:, 3:, :].permute(0, 2, 1)=tensor[256, 48, 1] f16 n=12288 x∈[-0.115, 0.072] μ=0.032 σ=0.019 grad PermuteBackward0 cuda:0\n",
      "transformed_features[FieldHeadNames.RGB].shape=torch.Size([256, 48, 3])\n",
      "transformed_features[FieldHeadNames.DENSITY].shape=torch.Size([256, 48, 1])\n"
     ]
    }
   ],
   "source": [
    "transformed_features = unet(features)\n",
    "# print(f\"{transformed_features.shape=}\")\n",
    "print(f\"{transformed_features[FieldHeadNames.RGB].shape=}\")\n",
    "print(f\"{transformed_features[FieldHeadNames.DENSITY].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed_features[FieldHeadNames.RGB]=tensor[256, 48, 3] f16 n=36864 x∈[0.485, 0.646] μ=0.554 σ=0.020 grad SigmoidBackward0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"{transformed_features[FieldHeadNames.RGB]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed_features[FieldHeadNames.DENSITY]=tensor[256, 48, 1] f16 n=12288 x∈[0., 0.121] μ=0.011 σ=0.016 grad ReluBackward0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"{transformed_features[FieldHeadNames.DENSITY]=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.models.nesf import FeatureGeneratorTorch\n",
    "\n",
    "fg = FeatureGeneratorTorch(model.scene_box.aabb).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[1, 1024, 48] n=49152 x∈[-1.000, 1.000] μ=0.182 σ=0.489 grad UnsqueezeBackward0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "features, weights, density_mask = fg(ray_bundle.to(\"cuda\"), model.to(\"cuda\"))\n",
    "features = features[:1024].unsqueeze(0)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.models.nesf import TransformerModel\n",
    "\n",
    "feature_transformer = TransformerModel(\n",
    "            output_size=6,\n",
    "            num_layers=2,\n",
    "            d_model=fg.get_out_dim(),\n",
    "            num_heads=4,\n",
    "            dff=64,\n",
    "            dropout_rate=0.1).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 6])\n"
     ]
    }
   ],
   "source": [
    "transformed_features = feature_transformer(features)\n",
    "print(transformed_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1572864, 3]              --\n",
       "├─Linear: 1-1                            [1572864, 128]            512\n",
       "├─ReLU: 1-2                              [1572864, 128]            --\n",
       "├─Linear: 1-3                            [1572864, 256]            33,024\n",
       "├─ReLU: 1-4                              [1572864, 256]            --\n",
       "├─Linear: 1-5                            [1572864, 256]            65,792\n",
       "├─ReLU: 1-6                              [1572864, 256]            --\n",
       "├─Linear: 1-7                            [1572864, 128]            32,896\n",
       "├─ReLU: 1-8                              [1572864, 128]            --\n",
       "├─Linear: 1-9                            [1572864, 3]              387\n",
       "├─Sigmoid: 1-10                          [1572864, 3]              --\n",
       "==========================================================================================\n",
       "Total params: 132,611\n",
       "Trainable params: 132,611\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 208.58\n",
       "==========================================================================================\n",
       "Input size (MB): 18.87\n",
       "Forward/backward pass size (MB): 9701.43\n",
       "Params size (MB): 0.53\n",
       "Estimated Total Size (MB): 9720.83\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# summary(fg, input_data=[ray_bundle, model])\n",
    "summary(fg.linear, input_size=((1 << 15)*48, 3), mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "UNet                                          [2048, 48, 1]             --\n",
       "├─Encoder: 1-1                                [2048, 16, 48, 32]        --\n",
       "│    └─ModuleList: 2-3                        --                        (recursive)\n",
       "│    │    └─Block: 3-1                        [2048, 16, 48, 32]        2,544\n",
       "│    └─MaxPool2d: 2-2                         [2048, 16, 24, 16]        --\n",
       "│    └─ModuleList: 2-3                        --                        (recursive)\n",
       "│    │    └─Block: 3-2                        [2048, 32, 24, 16]        14,016\n",
       "│    └─MaxPool2d: 2-4                         [2048, 32, 12, 8]         --\n",
       "├─Decoder: 1-2                                [2048, 16, 48, 32]        --\n",
       "│    └─ModuleList: 2-5                        --                        --\n",
       "│    │    └─ConvTranspose2d: 3-3              [2048, 16, 48, 32]        2,064\n",
       "│    └─ModuleList: 2-6                        --                        --\n",
       "│    │    └─Block: 3-4                        [2048, 16, 48, 32]        7,008\n",
       "├─Conv2d: 1-3                                 [2048, 4, 48, 32]         68\n",
       "├─ReLU: 1-4                                   [2048, 48, 1]             --\n",
       "===============================================================================================\n",
       "Total params: 25,700\n",
       "Trainable params: 25,700\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 47.27\n",
       "===============================================================================================\n",
       "Input size (MB): 12.58\n",
       "Forward/backward pass size (MB): 4529.85\n",
       "Params size (MB): 0.10\n",
       "Estimated Total Size (MB): 4542.53\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nerfstudio.models.nesf import FeatureGeneratorTorch, UNet\n",
    "from torchinfo import summary\n",
    "\n",
    "unet = UNet().to(\"cuda\")\n",
    "\n",
    "summary(unet, input_size=(2048, 48, 32), mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d982505c67f6491cc57124614a47f97cb6b2fba9cbe418d2edc6b5ed45f83d2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
